Due to the monitoring models requiring fast response times and the need for placement directly onto independent circuitry, it was essential to optimize the models for embedded systems. The original Python models, while suitable for prototyping, were converted into C code to demonstrate their feasibility for execution on resource-constrained embedded platforms. Python, although flexible, does not provide the computational efficiency necessary for these systems, particularly with respect to memory usage and execution speed. To reduce compute time and program footprint, the models were quantized. The quantization took the 64-bit floating-point values and converted them to 32-bit integer values through the use of a scaling factor to maintain computation precision. The quantized C models saw no accuracy degradation when compared to the original Python models while achieving an approximate 8-fold speedup in compute time. No direct implementation was shown for a specific microcontroller due to the wide range of equal options on the market. Instead, instruction counts are provided for each program to show the relative loads of each algorithm. This approach allows the models to be easily adapted to various microcontrollers and embedded systems.